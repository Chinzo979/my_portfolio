{
    "computer-vision": {
        "title": "Street-Level Object Detection",
        "intro": "The project I was assigned as a Computer Vision Co-op Engineer, at AirWorks Solutions (2024), was focused on detecting and classifying street-level telecommunication infrastructure from panoramic imagery. This process involved transforming over 30GB of raw data into structured insights. I was responsible for the entire end-to-end workflow — setting up the infrastructure (NVIDIA drivers, CUDA, and Dockerized environments), expanding and curating the dataset, preprocessing panoramic imagery, training and fine-tuning detection models, and finally deploying, testing, and evaluating model performance through inference pipelines.",
        "design": "The pipeline began with panorama preprocessing. Using inverse affine and homography matrices (via Equirec2Perspec and OpenCV remap), I un-stitched panoramic images into flattened segments for improved interpretability. This process required experimenting with numerous combinations of Field of View (FOV), Theta (horizontal angle), and Phi (vertical angle) values to achieve the most effective flattened perspectives. Metadata like GPS coordinates, pixel exposure distributions, and RGB channel statistics were extracted with cv2, NumPy, and pandas, and finally visualized in ArcGIS for geospatial analysis. For dataset expansion, I combined proprietary Teleqo and Tilson panoramas with public datasets (Roboflow, COCO format). I then wrote generalizable python scripts to merge annotations while ensuring category consistency, growing labeled instances from 291 to 16,525 across 10 infrastructure classes (Poles, Copper Splices, Risers, Fibre Loops, Ports, Power Boxes, etc.). I also used Label Studio, deployed via a Dockerized container, to manually annotate on images with poor resolution.",
        "implementation": "The first model I trained was Detectron2. It was loaded with pre-trained weights, a batch size of 128, and a learning rate of 0.0001, running for 270,000 iterations on an NVIDIA L4 GPU. Given my limited access to resources as an intern (GPU usage), I chose these parameter values through transfer learning - leveraging existing models to accelerate convergence and improve performance on the smaller, domain-specific dataset. I applied random rotations, brightness/contrast/hue/saturation adjustments, and resizing augmentations to introduce variance to the dataset. Metrics (loss, accuracy) were logged every 20 iterations into JSON trackers. However, Detectron2 used axis-aligned boxes instead of oriented bounding boxws (OBB). So, for improved precision on irregular geometries/tilted objects, I found NVIDIA’s Object Detection Toolkit (ODTK) model - https://developer.nvidia.com/blog/detecting-rotated-objects-using-the-odtk/. This new model was adopted with a ResNet152FPN backbone, rotated bounding boxes, L2 regularization, and fine-tuning from RetinaNet. Batch size was reduced to 8 to handle higher-resolution inputs (768 px).",
        "results": "Detectron2 achieved ~92% accuracy on the merged dataset but often prioritized background regions due to its axis-aligned boxes. Switching to NVIDIA’s ODTK with rotated bounding boxes produced tighter, orientation-aware localization (~78% accuracy across classes) and reduced spillover into background, especially around poles, risers, and cylindrical fiber storage loops. Flattening panoramas into segments improved instance separation and label quality, while dataset expansion (from 291 to 16,525 annotations) noticeably increased robustness and reduced overfitting on small samples (~950 images). Training/validation logs captured consistent loss reductions and steadier precision/recall trends after adopting rotated boxes and targeted augmentations. I transferred the trained artifacts, data-processing scripts, and deployment notes to my superior, Arjun, who integrated the pipeline into AirWorks production systems.",
        "future": "Naturally — given my constraints as an intern — the next step would be to scale training. This means using a larger, more diverse dataset and run for more iterations, paired with measured hyperparameter sweeps and stronger augmentations. I'd also experiment with deep-learning–based preprocessing (e.g., super-resolution, denoising, or edge-aware transforms) to sharpen small features and boost detection accuracy. On the deployment side, I’d streamline the ODTK pipeline, convert models to TensorRT for faster inference, and add monitoring for drift and latency with an active-learning loop from Label Studio to keep improving labels. And if data allows, incorporating rotated boxes end-to-end, modest temporal/3D cues, and lightweight model distillation for edge devices would further improve both accuracy and real-world fit."
    },
    "anime-recommender": {
        "title": "Anime Recommender System",
        "intro": "What fascinates me about Computer Science is recommendation algorithms. Programs built to understand human taste is the reason why people love Netflix, Spotify, TikTok, Instagram etc. The list is endless. As an anime fan, I struggled to find a platform that recommended titles based on my preferences, so I thought it would be cool to build an anime recommendation engine using collaborative filtering, content-based models, and hybrid blending on MyAnimeList data. The system powers personalized suggestions through a dashboard with type-safe Axios clients, protected routes, and skeleton loading states. It supports login-gated recommendation feeds, search with fuzzy matching, filtering based on genre/tags, metadata for each anime, and shareable links for any recommendation list.",
        "design": "The first thing any recommendation engine needs is data. To get started, I wrote scheduled Python scripts using MAL/Jikan RESTAPI (with rate limiting, exponential backoff, and idempotent checkpoints) to pull for user ratings, anime metadata, genres/demographics, studio info, and other missing tags. Raw payloads were validated with pydantic models, normalized (title casing, Unicode cleanup, UTC timestamps), and loaded into PostgreSQL (entities: anime, genres/themes, studios, users; bridges: anime_genre, user_anime). Then using feature engineering, I built a training table per user: (a) multi-hot encodings from genres/demographics/themes via scikit-learn’s MultiLabelBinarizer, (b) item stats (mean score, popularity rank, member count), (c) user history features (genre frequency, recency-weighted watch counts), and (d) simple text signals from synopses using TF–IDF (unigram/bigram, stopword removal). The recommendation happened in stages. The first-stage model is a content/metadata classifier that scores relevance of an (user, anime) pair. The second stage is a lightweight re-ranker that boosts items overlapping a user’s top genres and penalizes duplicates from the same franchise. In case of a cold start, the model falls back to popularity- and genre-personalized lists and build from there. Note, that the model assigned per user is retrained after every 10 new anime title additions.",
        "implementation": "Everything runs in Python 3.9+ with pandas/NumPy for ETL and scikit-learn for modeling. The main recommender is a RandomForestClassifier trained on implicit feedback labels (watched = positive; a stratified sample of unwatched = negative) per user segment, with class weights to counter imbalance. Hyperparameters are tuned with randomized search over trees/depth/max_features; models are persisted with joblib and tagged with a data+code hash for reproducibility. A nightly cron (or GitHub Actions scheduled workflow) refreshes the feature store and retrains user segments based on new additions. A smaller incremental job updates recent interactions and refreshes the top-N cache. The service stack is simple: a Streamlit UI (login-gated) for search and top-N recommendations, a FastAPI endpoint for programmatic access, and PostgreSQL as the truth source as well as a denormalized recommendations table (indexes on (user_id, score DESC)). The UI also supports fuzzy title search and per-genre filters. Tests cover validators, feature builders, and ranking invariants (pytest + coverage); pre-commit hooks enforce black/isort/flake8. CI on GitHub Actions runs unit tests and lints on every push; successful builds upload fresh model artifacts as release assets. Basic observability comes from structured logs (JSON), timing middleware around prediction calls, and simple health/metrics endpoints (request rate, cache hit rate, p95 latency).",
        "results": "The model evaluation is offline and reproducible. I constructed time-based splits (train on older interactions, test on newer) to avoid leakiness, then compute precision, recall, and hit rate for k ∈ {5,10,20}. I also track coverage (unique items recommended) and catalog drift across refreshes. Results are recorded per run in the repo’s artifacts (model hash, feature snapshot, metrics) so numbers on the site can be verified against a specific build. In qualitative tests with a small beta group, users preferred genre-diverse slates with short explanations over popularity-only lists; internal logs show low double-click rates on duplicates from the same franchise after re-rank rules were added. Latency under typical load is dominated by DB reads and feature lookup; with a cached top-N table and in-process LRU for recent users, interactive requests return quickly on commodity hardware.",
        "future": "This started off as a personal project, but I would love to expand it to a production-ready system. Existing websites like crunchyroll still use top-10 lists, so there is still room for improvement. As far as this project is concerned, near term improvements include adding redis caching for hot users/items; introduce per-user incremental updates so new watches are reflected without a full retrain; expanding the dataset of animes from 2000+ to 10000+ titles. Medium term: incorporate lightweight sentiment from user reviews to modulate scores, scale from 1-10 to allow app users to more granularly rate animes, and expose a minimal SDK for batch export/embedding on a portfolio page. Longer term: explore hybridization with a matrix-factorization or shallow representation model, add guardrails for popularity bias and over-narrow personalization, and wire up a scheduled evaluation dashboard that surfaces metric deltas by segment after each data refresh."
    },
    "search-engine": {
        "title": "StreamLine: Smart Calendar for the ADHD Community",
        "intro": "As part of EECS 495 (Accessible Computing) in Spring 2024, I led a team and built StreamLine — a Google Calendar and Canvas wrapper designed to reduce planning overhead for students with ADHD. The idea was born when me and my teammate, Chun-Yu, were discussing how we could help his girlfriend, who has ADHD, manage her time better. I primarily focused on automating event creation, centralizing academic and personal schedules, and making minute-level planning fast and forgiving. I co-designed the app with our client, Chun-Yu's girlfriend, through weekly feedback to keep the UI and features aligned with her needs.",
        "design": "I chose to augment — not replace — Google Calendar, so events created in StreamLine appear alongside existing calendar items. I added presets for common routines (e.g., medication, study, sleep), views for day/week/month/list, and adjustable time intervals down to 1-minute granularity for precise planning. This was an intentional choice since individuals  with ADHD function a lot better when presented with tasks in smaller time intervals. Next, I integrated Canvas, a common learning management system found in high schools and universities, to surface upcoming assignments with due dates. The feature me and my teammates are most proud of was building an 'Auto Place' feature that finds the next open time slot and schedules an event automatically. In the case of multiple open spots, the user could choose where to place the event. My teammates Hazel and Alex then refined the UI based on our client’s preferences (theme updates, a settings sidebar, background customization) and emphasized quick actions over typing to counter time-blindness and decision fatigue.",
        "implementation": "The app was built in Python with Streamlit for the UI, Google OAuth for sign-in, the Google Calendar API for creating and updating events, and the Canvas API for importing assignments. I defined simple event schemas (title, start, end, description, full-day, overlap behavior) and wrote handlers for add/delete flows. To fix assignment timing drift, I normalized date-times with datetime and pytz. I also moved noisy debug output into a file logger so the interface stayed clean. Streamlit’s link-embedding limitations shaped the first version of Canvas links (rendered as text in event descriptions), which I documented for a future front-end refactor.",
        "results": "My team and I successfully delivered a working prototype that authenticated with Google, created calendar events through the app, and imported course assignments from Canvas into the same timeline. 'Auto Place' reduced the friction of finding time; presets made common routines one-click. After a round of bug fixes (notably time-zone mismatches and UI logging), the app reliably synchronized events and reflected our client’s day-to-day needs. Client feedback highlighted easier scheduling, better visibility of schoolwork, and enthusiasm for additional automation.",
        "future": "Since this was designed as a proof of concept and for a specific client, the next natural step would be to generalize the app so it can be used by other students with ADHD. I plan to scale automation: smarter presets and richer rules (e.g., preferred windows, conflict strategies), interactive assignment links, and lighter-weight components beyond Streamlit. I also want to add deep-learning/NLP helpers (voice or natural-language event creation), optimize performance, and enable more APIs so StreamLine can grow beyond student workflows while keeping an accessible, low-friction UI. Lastly, integrating the calendar with more thirdy-party API services like google keep, google tasks, etc. could make the app even more useful."
    }
}